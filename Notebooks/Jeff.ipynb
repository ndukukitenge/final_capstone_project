{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Machine Learning for Social Media-Based Depression Analysis** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mental health is an urgent issue globally, with depression affecting millions of individuals across all demographics. \n",
    "The internet, and particularly social media, has become a place where people often express their struggles, including depression. \n",
    "Early detection of depression symptons can be crucial in providing timely support or intervention. This project’s goal is to leverage data science to better understand and detect depressive expressions in online platforms, potentially paving the way for more proactive mental health support. Given the widespread use of platforms like Reddit, this research could benefit individuals by increasing awareness and intervention opportunities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasource: https://www.kaggle.com/datasets/rishabhkausish/reddit-depression-dataset/data \n",
    "\n",
    "The dataset already includes several key features that can be used to analyze and predict depression indicators based on Reddit posts. Specifically, the data has 7 key columns:\n",
    "\n",
    "\n",
    "**subreddit** : The subreddit where each post was made, with posts from *\"Depression\"* and *\"SuicideWatch\"* labeled as 1 for depression and posts from other subreddits labeled as 0 (non-depression).\n",
    "\n",
    "**title**: The title of the Reddit post.\n",
    "\n",
    "**body**: The full text of the Reddit post, which may contain valuable information for understanding the context, tone, and possible indicators of depression.\n",
    "\n",
    "**upvotes**: Number of upvotes each post received, which may indicate the post's visibility or resonance with the community.\n",
    "\n",
    "**created_utc**: The timestamp of when the post was created in UTC, which can help in analyzing temporal trends.\n",
    "\n",
    "**num_comments**: The number of comments on each post, which could provide insights into community engagement.\n",
    "\n",
    "**label**: The target variable indicating depression (1) or non-depression (0) based on subreddit.\n",
    "....\n",
    "The raw data was collected  from five Reddit subreddits (sub topics), categorized based on their content. These included: Teenagers, Depression, SuicideWatch, DeepThoughts, Happy\n",
    "\n",
    "Since the data is already collected from Reddit, with over 6 million rows, further data acquisition may not be necessary. Infact, the team proposes reducing the dataset to about 500,000 rows for the purpose of this project(and to save our laptops). However, if additional data is needed, we could plan to scrape Reddit for more recent posts using a tool that we could identify through further research, provided we comply with Reddit's data collection policies and privacy standards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was reduced since it was too huge to work with.The data was reduced to 98826 features with 6 columns.there are null values in body and num_comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "np.random.seed(0)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>Day 18 of doing 50 push-ups</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.619357e+09</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>There isn’t a better feeling than finishing yo...</td>\n",
       "      <td>Then your teacher hits you with that “ Good jo...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.554103e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>LMAOOO I can only get this guy to talk to me i...</td>\n",
       "      <td>Yeahhh maybe not babe</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.649342e+09</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>This isn't going to work out</td>\n",
       "      <td>NaN</td>\n",
       "      <td>236.0</td>\n",
       "      <td>1.417630e+09</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>Am I the only guy who found it hurtful as a ch...</td>\n",
       "      <td>\\n\\nLike... why? How is that funny? How does ...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.569280e+09</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98821</th>\n",
       "      <td>depression</td>\n",
       "      <td>Has anyone ever had any problems with gangs?</td>\n",
       "      <td>I don't care if my grammer isn't good so dont ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.558082e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98822</th>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>Moments away from killing myself, not even bot...</td>\n",
       "      <td>I don't want to talk about my problems, I don'...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.350754e+09</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98823</th>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>I've grown used to being like this for years.</td>\n",
       "      <td>I'm not suicidal at the moment, in fact I feel...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.432074e+09</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98824</th>\n",
       "      <td>depression</td>\n",
       "      <td>This speech from The Lord of the Rings really ...</td>\n",
       "      <td>\"It's like in the great stories, Mr. Frodo. Th...</td>\n",
       "      <td>323.0</td>\n",
       "      <td>1.355673e+09</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98825</th>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>I just don't know....</td>\n",
       "      <td>I have never thought that I will end up in thi...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.328355e+09</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98826 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit                                              title  \\\n",
       "0         teenagers                        Day 18 of doing 50 push-ups   \n",
       "1         teenagers  There isn’t a better feeling than finishing yo...   \n",
       "2         teenagers  LMAOOO I can only get this guy to talk to me i...   \n",
       "3         teenagers                       This isn't going to work out   \n",
       "4         teenagers  Am I the only guy who found it hurtful as a ch...   \n",
       "...             ...                                                ...   \n",
       "98821    depression       Has anyone ever had any problems with gangs?   \n",
       "98822  SuicideWatch  Moments away from killing myself, not even bot...   \n",
       "98823  SuicideWatch      I've grown used to being like this for years.   \n",
       "98824    depression  This speech from The Lord of the Rings really ...   \n",
       "98825  SuicideWatch                              I just don't know....   \n",
       "\n",
       "                                                    body  upvotes  \\\n",
       "0                                                    NaN      4.0   \n",
       "1      Then your teacher hits you with that “ Good jo...      7.0   \n",
       "2                                  Yeahhh maybe not babe      4.0   \n",
       "3                                                    NaN    236.0   \n",
       "4       \\n\\nLike... why? How is that funny? How does ...      6.0   \n",
       "...                                                  ...      ...   \n",
       "98821  I don't care if my grammer isn't good so dont ...      8.0   \n",
       "98822  I don't want to talk about my problems, I don'...     12.0   \n",
       "98823  I'm not suicidal at the moment, in fact I feel...     14.0   \n",
       "98824  \"It's like in the great stories, Mr. Frodo. Th...    323.0   \n",
       "98825  I have never thought that I will end up in thi...      5.0   \n",
       "\n",
       "        created_utc  num_comments  label  \n",
       "0      1.619357e+09           4.0    0.0  \n",
       "1      1.554103e+09           NaN    0.0  \n",
       "2      1.649342e+09          12.0    0.0  \n",
       "3      1.417630e+09          33.0    0.0  \n",
       "4      1.569280e+09           4.0    0.0  \n",
       "...             ...           ...    ...  \n",
       "98821  1.558082e+09           1.0    1.0  \n",
       "98822  1.350754e+09           8.0    1.0  \n",
       "98823  1.432074e+09           4.0    1.0  \n",
       "98824  1.355673e+09          28.0    1.0  \n",
       "98825  1.328355e+09           3.0    1.0  \n",
       "\n",
       "[98826 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\user\\Documents\\final_project\\Data\\reduced_reddit.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 98826 entries, 0 to 98825\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   subreddit     98826 non-null  object \n",
      " 1   title         98826 non-null  object \n",
      " 2   body          80371 non-null  object \n",
      " 3   upvotes       98826 non-null  float64\n",
      " 4   created_utc   98826 non-null  float64\n",
      " 5   num_comments  94297 non-null  float64\n",
      " 6   label         98826 non-null  float64\n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "#there are a total of 98826 features,null values are in body and num_comment\n",
    "#we will drop null values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 77051 entries, 2 to 98825\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   subreddit     77051 non-null  object \n",
      " 1   title         77051 non-null  object \n",
      " 2   body          77051 non-null  object \n",
      " 3   upvotes       77051 non-null  float64\n",
      " 4   created_utc   77051 non-null  float64\n",
      " 5   num_comments  77051 non-null  float64\n",
      " 6   label         77051 non-null  float64\n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df=df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26852\\3471966554.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['title'] = df['title'].str.lower().copy()\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26852\\3471966554.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col]=df[col].str.lower()\n"
     ]
    }
   ],
   "source": [
    "#lower casing the text\n",
    "df['title'] = df['title'].str.lower().copy()\n",
    "low_column=df[['subreddit','body']]\n",
    "for col in low_column:\n",
    "    df[col]=df[col].str.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "def preprocessing_text(text):\n",
    "    #remove punctuations\n",
    "    text = text.translate(str.maketrans('','', string.punctuation))\n",
    "    #remove numbers\n",
    "    text= re.sub(r'd/+', '',text)\n",
    "    #remove stopwords\n",
    "    stop_word=set(stopwords.words('english'))\n",
    "    text=' '.join([word for word in text.split() if word not in stop_word])\n",
    "    #tokenization\n",
    "    tokens=word_tokenize(text)\n",
    "    #lemmatization\n",
    "    lemitizer= WordNetLemmatizer()\n",
    "    tokens= [lemitizer.lemmatize(word) for word in tokens]\n",
    "    #rejoin token\n",
    "    text=' '.join(tokens)\n",
    "    #remove extra whitespace\n",
    "    text=' '.join(text.split())\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26852\\2744839373.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['title'] = df['title'].apply(preprocessing_text)\n"
     ]
    }
   ],
   "source": [
    "#preprocess tittle column\n",
    "df['title'] = df['title'].apply(preprocessing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26852\\274056569.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['subreddit'] = df['subreddit'].apply(preprocessing_text)\n"
     ]
    }
   ],
   "source": [
    "#preprocess sureddit column\n",
    "df['subreddit'] = df['subreddit'].apply(preprocessing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26852\\1656550955.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['body'] = df['body'].apply(preprocessing_text)\n"
     ]
    }
   ],
   "source": [
    "#preprocess body column\n",
    "df['body'] = df['body'].apply(preprocessing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26852\\3838996466.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['title']=df['title'].apply(remove_emoji)\n"
     ]
    }
   ],
   "source": [
    "#removing emojis from the title column\n",
    "import emoji\n",
    "def remove_emoji(text):\n",
    "    return emoji.replace_emoji(text,'')\n",
    "\n",
    "df['title']=df['title'].apply(remove_emoji)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.5 GiB for an array with shape (77051, 21745) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m tf_idf_vectorizer\u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m      6\u001b[0m tf_idf_vectorizer_matrix\u001b[38;5;241m=\u001b[39m tf_idf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m vect_array\u001b[38;5;241m=\u001b[39m\u001b[43mtf_idf_vectorizer_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(vect_array)\n\u001b[0;32m      9\u001b[0m vect_names\u001b[38;5;241m=\u001b[39mtf_idf_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1051\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\sparse\\_base.py:1298\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 12.5 GiB for an array with shape (77051, 21745) and data type float64"
     ]
    }
   ],
   "source": [
    "# we will encode the texts \n",
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vectorizer= TfidfVectorizer()\n",
    "tf_idf_vectorizer_matrix= tf_idf_vectorizer.fit_transform(df['title'])\n",
    "vect_array=tf_idf_vectorizer_matrix.toarray()\n",
    "print(vect_array)\n",
    "vect_names=tf_idf_vectorizer.get_feature_names_out()\n",
    "print(vect_array)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.5 GiB for an array with shape (77051, 21745) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vect_array,vect_names\u001b[38;5;241m=\u001b[39m\u001b[43mword_vectorization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF-IDF Matrix:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(vect_array)\n",
      "Cell \u001b[1;32mIn[39], line 7\u001b[0m, in \u001b[0;36mword_vectorization\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m tf_idf_vectorizer\u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m      6\u001b[0m tf_idf_vectorizer_matrix\u001b[38;5;241m=\u001b[39m tf_idf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(text)\n\u001b[1;32m----> 7\u001b[0m vect_array\u001b[38;5;241m=\u001b[39m\u001b[43mtf_idf_vectorizer_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m vect_names\u001b[38;5;241m=\u001b[39mtf_idf_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vect_array,vect_names\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1051\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\sparse\\_base.py:1298\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 12.5 GiB for an array with shape (77051, 21745) and data type float64"
     ]
    }
   ],
   "source": [
    "vect_array,vect_names=word_vectorization(df['title'])\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(vect_array)\n",
    "\n",
    "print(\"Feature Names (Terms):\")\n",
    "print(vect_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF matrix contains mostly zero values, and the feature names (terms) include a mix of regular words and potentially unusual characters (e.g., 𝚜𝚝𝚞𝚏𝚏, 𝟒𝟖𝟕).\n",
    "The TF-IDF matrix is sparse, meaning that most of the entries are zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Check the first 5 rows and columns of the dense matrix\n",
    "print(vect_array[:5, :5])  # Print first 5 rows and columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im: 1462.6040721472018\n",
      "like: 1071.6317044709272\n",
      "guy: 1024.0256835953255\n",
      "want: 918.0257363886897\n",
      "help: 917.5183239052286\n",
      "need: 814.8536111144069\n",
      "girl: 805.8965026594711\n",
      "day: 800.9235189004057\n",
      "friend: 788.8152387198311\n",
      "get: 775.9006921066967\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Sum up the TF-IDF scores across all documents for each word\n",
    "tfidf_sums = np.sum(vect_array, axis=0)\n",
    "\n",
    "# Get the top N indices of the terms with the highest TF-IDF scores\n",
    "top_n = 10  # Adjust N as needed\n",
    "top_n_indices = np.argsort(tfidf_sums)[::-1][:top_n]\n",
    "\n",
    "# Get the top N words based on their TF-IDF sum\n",
    "top_n_words = [vect_names[i] for i in top_n_indices]\n",
    "\n",
    "# Print the top N words and their corresponding TF-IDF scores\n",
    "top_n_scores = tfidf_sums[top_n_indices]\n",
    "for word, score in zip(top_n_words, top_n_scores):\n",
    "    print(f\"{word}: {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

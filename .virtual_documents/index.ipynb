











#import libraries needed
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA



from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier


#Load the dataset to use 

reduced_data = pd.read_csv('Data/reduced_reddit.csv', index_col=False)
reduced_data.head(10) # Display the first few rows of the dataframe







#check the dataset information 
reduced_data.info()





#check on the description of numerical datatypes
reduced_data.describe()








#check to see if there are null values in percentage form
reduced_data.isnull().sum() / len(data) * 100





# Now you can fill missing values in 'body' column 
reduced_data['num_comments'] =reduced_data['num_comments'].fillna(0)

# Check the DataFrame info again to confirm the change
reduced_data.info() 





reduced_data.isnull().sum()





# Fill missing values in the 'body' column
reduced_data['body'] = reduced_data['body'].fillna(" ")

# Save the cleaned dataset into a new DataFrame
cleaned_data = reduced_data.copy()





#check for missing values 
cleaned_data.isnull().sum()






# Convert 'created_utc' column from UTC epoch time to datetime
cleaned_data['created_utc'] = pd.to_datetime(cleaned_data['created_utc'], unit='s')

# Rename the column 'created_utc' to 'date'
cleaned_data.rename(columns={'created_utc': 'date'}, inplace=True)

# To see the updated DataFrame
cleaned_data.head()






cleaned_date_date = cleaned_data.copy()
cleaned_date_date





#check for missing values 
cleaned_date_date.isnull().sum()





# Import necessary libraries
from wordcloud import WordCloud


# Combine the title and body columns into a single text string
text = ' '.join(cleaned_data['title'].fillna('') + ' ' + cleaned_data['body'].fillna(''))

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Turn off axis
plt.show()



# wordcloud for combined title and body columns
text = ' '.join(cleaned_data['title'].astype(str) + ' ' + cleaned_data['body'].astype(str))

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()


#word_cloud(data['subreddit'])


#word_cloud(data['body'])


# Upvotes box plot
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 2)
cleaned_data['upvotes'].plot(kind='box')
plt.title('Upvotes Box Plot')
plt.show()

#for num_comments
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 2)
cleaned_data['num_comments'].plot(kind='box')
plt.title('Num Comments Box Plot')
plt.show()






#Frequency of categorical columns

# Frequency of 'subreddit' column
print(cleaned_data['subreddit'].value_counts())

# Frequency of 'label' column
print(cleaned_data['label'].value_counts())

# Set style
sns.set(style="whitegrid")

# Plot the counts for 'subreddit' column
plt.figure(figsize=(12, 6))
sns.countplot(data=cleaned_data, y='subreddit', order=cleaned_data['subreddit'].value_counts().index, palette='viridis')
plt.title('Counts of Posts by Subreddit')
plt.xlabel('Count')
plt.ylabel('Subreddit')
plt.show()

# Plot the counts for 'label' column
plt.figure(figsize=(8, 6))
sns.countplot(data=cleaned_data, x='label', palette='coolwarm')
plt.title('Counts by Label')
plt.xlabel('Label')
plt.ylabel('Count')
plt.xticks(ticks=[0, 1], labels=['Non-depressed', 'Depressed'])
plt.show()






# Plot number of posts over time

cleaned_data.set_index('date', inplace=True)

# Plot number of posts per month
cleaned_data.resample('M').size().plot(figsize=(12, 6))
plt.title('Number of Posts Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Posts')
plt.show()














#Numerical vs. Numerical (Scatter plot and correlation matrix)
#Scatter Plot - Shows the relationship between two numerical variables, upvotes and num_comments.

# Scatter plot for numerical variables
plt.figure(figsize=(8, 6))
sns.scatterplot(x='upvotes', y='num_comments', data=cleaned_data, alpha=0.5)
plt.title("Scatter Plot of Upvotes vs. Number of Comments")
plt.xlabel("Upvotes")
plt.ylabel("Number of Comments")
plt.show()

# Correlation matrix and heatmap
correlation_matrix = cleaned_data[['upvotes', 'num_comments']].corr()
plt.figure(figsize=(6, 5))
sns.heatmap(correlation_matrix, annot=True, cmap='Spectral', square=True)
plt.title("Correlation Matrix of Numerical Variables")
plt.show()





# Numerical vs. Categorical (Box plot and Violin plot)
# Box plot of 'upvotes' by 'label'
plt.figure(figsize=(8, 6))
sns.boxplot(x='label', y='upvotes', data=cleaned_data, palette='pastel')
plt.title("Box Plot of Upvotes by Label")
plt.xlabel("Label")
plt.ylabel("Upvotes")
plt.xticks(ticks=[0, 1], labels=["Non-Suicidal (0)", "Suicidal (1)"])
plt.show()

# Violin plot of 'num_comments' by 'subreddit'
plt.figure(figsize=(10, 6))
sns.violinplot(x='subreddit', y='num_comments', data=cleaned_data, palette='muted')
plt.title("Violin Plot of Number of Comments by Subreddit")
plt.xlabel("Subreddit")
plt.ylabel("Number of Comments")
plt.show()


# Crosstab between 'subreddit' and 'label'
crosstab = pd.crosstab(cleaned_data['subreddit'], cleaned_data['label'])
print("Crosstab of Subreddit by Label:")
print(crosstab)

# Bar plot for 'subreddit' and 'label'
plt.figure(figsize=(10, 6))
sns.countplot(data=cleaned_data, x="subreddit", hue="label", palette="Set2")
plt.title("Subreddit Distribution by Label")
plt.xlabel("Subreddit")
plt.ylabel("Count")
plt.legend(title="Label", loc='upper right', labels=["Non-depressed (0)", "Depressed (1)"])
plt.show()





# Pair plot with 'label' as the hue
#sns.pairplot(data[['upvotes', 'num_comments', 'date', 'label']], hue='label', palette='Set1', diag_kind='kde')
#plt.suptitle("Pair Plot of Numerical Variables by Label", y=1.02)
#plt.show()


# Convert the 'subreddit' column to numeric using label encoding
cleaned_data['subreddit_encoded'] = LabelEncoder().fit_transform(cleaned_data['subreddit'])

# Now include the encoded 'subreddit' in the heatmap
plt.figure(figsize=(8,6))
sns.heatmap(cleaned_data[['subreddit_encoded', 'upvotes', 'num_comments', 'label']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap with Encoded Subreddit')
plt.show()



# --- 1. Feature Creation ---

# Extract year, month, day, hour from 'date'
cleaned_date_date['year'] = cleaned_date_date['date'].dt.year
cleaned_date_date['month'] = cleaned_date_date['date'].dt.month
cleaned_date_date['day'] = cleaned_date_date['date'].dt.day
cleaned_date_date['hour'] = cleaned_date_date['date'].dt.hour

# Create additional features like ratios or flags
cleaned_date_date['upvotes_per_comment'] = cleaned_date_date['upvotes'] / (cleaned_date_date['num_comments'] + 1)  # Adding 1 to avoid division by zero
cleaned_date_date['has_body'] = cleaned_date_date['body'].apply(lambda x: 0 if x == ' ' else 1)  # Flag indicating if there's content in 'body'

# --- 2. Scaling and Normalization ---
# Initialize scalers
standard_scaler = StandardScaler()
minmax_scaler = MinMaxScaler()

# Apply Standard Scaling on numerical features
cleaned_date_date[['upvotes', 'num_comments', 'upvotes_per_comment']] = standard_scaler.fit_transform(
    cleaned_date_date[['upvotes', 'num_comments', 'upvotes_per_comment']]
)

# Apply Min-Max Scaling on year, month, day, hour (optional)
cleaned_date_date[['year', 'month', 'day', 'hour']] = minmax_scaler.fit_transform(
   cleaned_date_date[['year', 'month', 'day', 'hour']]
)

# Save the cleaned data to a new DataFrame
data = cleaned_date_date.copy()

# Display the final DataFrame
print(data.head())







data.info()











#We will use a pretrained sentiment analysis model --- for this case we will use Vader 
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Handle non-string values in the title column
data['title'] = data['title'].fillna("").astype(str)

# Initialize the SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

# Apply VADER sentiment analysis to the title column
data['sentiment_scores'] = data['title'].apply(lambda x: analyzer.polarity_scores(x))

# Categorize sentiment based on compound score
data['sentiment'] = data['sentiment_scores'].apply(
    lambda x: 'non depressed' if x['compound'] > 0 else 'depressed'
)

# Verify the result
data[['title', 'sentiment_scores', 'sentiment']].head()



# Display the first few rows of sentiment scores
data[['title', 'sentiment_scores']].head()


# Display the first few rows of the sentiment column
data[['subreddit','title', 'sentiment']].head()


# Count the number of positive and negative sentiments
sentiment_counts = data['sentiment'].value_counts()
print(sentiment_counts)


# Display a few positive examples
positive_examples = data[data['sentiment'] == 'non depressed']
positive_examples[['title', 'sentiment_scores', 'sentiment']].head()




# Display a few negative examples
negative_examples = data[data['sentiment'] == 'depressed']
negative_examples[['title', 'sentiment_scores', 'sentiment']].head()








#Text Preprocessing 

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.base import TransformerMixin, BaseEstimator
import string

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

# Initialize stopwords, punctuation, and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

#define a class TextPreprocessor that conforms to scikit-learn’s transformer API
class TextPreprocessor(BaseEstimator, TransformerMixin):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self  # No fitting required for this transformer

    def transform(self, X, y=None):
        return X.apply(self._preprocess_text)

    def _preprocess_text(self, text):
        if pd.isnull(text):
            return ''  # Return empty string for missing values
        text = text.lower()  # Lowercase
        text = ''.join([char for char in text if char not in string.punctuation])  # Remove punctuation
        tokens = word_tokenize(text)  # Tokenize
        tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords
        tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatize
        return ' '.join(tokens)  # Join tokens back into string


# Instantiate TextPreprocessor
text_preprocessor = TextPreprocessor()

# Apply the text preprocessor to 'title' and 'body' columns
data['processed_title'] = text_preprocessor.transform(data['title'])
data['processed_body'] = text_preprocessor.transform(data['body'])

# Display the processed data
data[['subreddit','processed_title', 'processed_body']]









# TF-IDF -give higher weights to words that are less common across documents.
# Import necessary libraries
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import csr_matrix


# Apply TF-IDF to the title and body columns separately
tfidf_vectorizer = TfidfVectorizer(max_features=10000)  # limit to top 10,000 features_to manage vocabulary size


# Use the processed title and body columns 
tfidf_title = tfidf_vectorizer.fit_transform(data['processed_title'])
tfidf_body = tfidf_vectorizer.fit_transform(data['processed_body'])

# You can keep these as sparse matrices or concatenate directly if needed for model input
combined_features = csr_matrix(tfidf_title) + csr_matrix(tfidf_body) # in sparse format, to save memory





#Examine the top words by TF-IDF Scores

# Display the top terms with the highest TF-IDF scores
tfidf_sum = combined_features.sum(axis=0)
feature_names = tfidf_vectorizer.get_feature_names_out()
tfidf_df = pd.DataFrame({'term': feature_names, 'tfidf_score': tfidf_sum.A1})
tfidf_df = tfidf_df.sort_values(by='tfidf_score', ascending=False)

# Display the top 20 terms with the highest TF-IDF scores
tfidf_df.head(20)





# Visualize the Top TF-IDF Terms

# Bar plot of the top 20 terms
plt.figure(figsize=(12, 8))
plt.barh(tfidf_df['term'].head(20), tfidf_df['tfidf_score'].head(20))
plt.xlabel("TF-IDF Score")
plt.title("Top 20 TF-IDF Terms")
plt.gca().invert_yaxis()
plt.show()








# Combine 'processed_title' and 'processed_body' columns 
data['processed_text'] = data['processed_title'] + " " + data['processed_body']
combined_text = " ".join(data['processed_text'].dropna().tolist())

# Create the word cloud
wordcloud = WordCloud(
    width=800, 
    height=400, 
    background_color='white',
    max_words=100,  # Limit the number of words
    colormap='viridis'  # colormap
).generate(combined_text)

# Display the word cloud
plt.figure(figsize=(10, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Turn off axis
plt.show()









#Select and train a model using features extracted in the previous step.
# Model 1 - Baseline Model 
#Logistic Regression from sklearn.model_selection import train_test_split

from imblearn.pipeline import Pipeline as ImbPipeline  # Use imbalanced-learn's Pipeline for compatibility with SMOTE
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report


# Combine the title and body text into one column
data['combined_text'] = data['title'].fillna('') + ' ' + data['body'].fillna('')

# Define the target and features
X = data[['combined_text']]
y = data['label']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define preprocessing: text vectorization only
preprocessor = ColumnTransformer([
    ('text', TfidfVectorizer(max_features=10000), 'combined_text')  # Text processing
])

# Define a pipeline with preprocessing, SMOTE, and logistic regression
pipeline = ImbPipeline([
    ('preprocessor', preprocessor),
    ('smote', SMOTE(random_state=42)),
    ('classifier', LogisticRegression(solver='saga', max_iter=200))
])

# Fit the pipeline on the training data
pipeline.fit(X_train, y_train)

# Evaluate on training data
train_predictions = pipeline.predict(X_train)
train_accuracy = accuracy_score(y_train, train_predictions)
print("Training Accuracy:", train_accuracy)

# Evaluate on testing data
test_predictions = pipeline.predict(X_test)
test_accuracy = accuracy_score(y_test, test_predictions)
print("Testing Accuracy:", test_accuracy)

# Print classification report for test set
print("Classification Report:\n", classification_report(y_test, test_predictions))







from sklearn.metrics import confusion_matrix


# Predictions on the test set
test_predictions = pipeline.predict(X_test)

# Compute the confusion matrix
cm = confusion_matrix(y_test, test_predictions)

# Visualize the confusion matrix using seaborn heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-depressed (0)', 'Depressed (1)'], 
            yticklabels=['Non-depressed (0)', 'Depressed (1)'])
plt.title('Confusion Matrix for Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()












from sklearn.model_selection import RandomizedSearchCV

# Define the pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', LogisticRegression(max_iter=1000))
])

# Define the hyperparameter distributions
param_dist = {
    'model__C': [0.01, 0.1, 1, 10],  # Regularization strength
    'model__penalty': ['l1', 'l2'],       # Regularization types
    'model__solver': ['liblinear', 'saga']  # Compatible solvers
}

# Instantiate the RandomizedSearchCV object
random_search = RandomizedSearchCV(
    pipeline, param_distributions=param_dist,
    n_iter=50, cv=5, n_jobs=-1, verbose=2, random_state=42
)

# Fit the RandomizedSearchCV object
random_search.fit(X_train, y_train)

# Get the best parameters and score
print("Best Parameters:", random_search.best_params_)
print("Best Cross-Validation Score:", random_search.best_score_)

# Evaluate on the test set
test_predictions = random_search.best_estimator_.predict(X_test)
test_accuracy = accuracy_score(y_test, test_predictions)
print("Testing Accuracy with Best Parameters:", test_accuracy)








from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.compose import ColumnTransformer
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report

# Combine the title and body text into one column
data['combined_text'] = data['title'].fillna('') + ' ' + data['body'].fillna('')

# Define the target and features
X = data[['combined_text']]
y = data['label']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define preprocessing: text vectorization only
preprocessor = ColumnTransformer([
    ('text', TfidfVectorizer(max_features=10000), 'combined_text')  # Text processing
])

# Define the models to use
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    #'SVM': SVC(kernel='linear', probability=True, random_state=42)
    
}

# Loop through each model and evaluate
for model_name, model in models.items():
    print(f"\nEvaluating model: {model_name}")

    # Define the pipeline
    pipeline = ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', SMOTE(random_state=42)),
        ('classifier', model)
    ])

    # Fit the pipeline on the training data
    pipeline.fit(X_train, y_train)

    # Evaluate on training data
    train_predictions = pipeline.predict(X_train)
    train_accuracy = accuracy_score(y_train, train_predictions)
    print(f"Training Accuracy ({model_name}):", train_accuracy)

    # Evaluate on testing data
    test_predictions = pipeline.predict(X_test)
    test_accuracy = accuracy_score(y_test, test_predictions)
    print(f"Testing Accuracy ({model_name}):", test_accuracy)

    # Print classification report for test set
    print(f"Classification Report ({model_name}):\n", classification_report(y_test, test_predictions))

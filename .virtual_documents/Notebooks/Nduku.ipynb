


# Import necessary libraries

import pandas as pd
import numpy as np
import string
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder
from sklearn.base import TransformerMixin, BaseEstimator
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from datetime import datetime
from wordcloud import WordCloud






df= pd.read_csv("C:/Users/User/OneDrive/Documents/Flatiron/Coursework/Phase 5 Capstone/FinalProject/final_project/Data/reduced_reddit.csv")
df.info()
df.isnull().sum()


df.head(5)


df.describe()



# Count unique subreddits
unique_subreddits = df['subreddit'].nunique()
print(f"Number of unique subreddits: {unique_subreddits}")

# List unique subreddits
subreddit_categories = df['subreddit'].unique()
print(f"Subreddit categories: {subreddit_categories}")



# Convert 'created_utc' from float to readable datetime
df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')


# Analyze label distribution by subreddit
label_analysis = df.groupby('subreddit')['label'].value_counts().unstack(fill_value=0)

# Show the analysis of labels by subreddit
print(label_analysis)


df.head(5)






# Histogram of upvotes
plt.figure(figsize=(10,6))
sns.histplot(df['upvotes'], kde=True, bins=20)
plt.title('Distribution of Upvotes')
plt.xlabel('Upvotes')
plt.ylabel('Frequency')
plt.show()

# Histogram of num_comments
plt.figure(figsize=(10,6))
sns.histplot(df['num_comments'], kde=True, bins=20)
plt.title('Distribution of Number of Comments')
plt.xlabel('Number of Comments')
plt.ylabel('Frequency')
plt.show()



# Bar plot for label distribution by subreddit
label_distribution = df.groupby('subreddit')['label'].value_counts().unstack().fillna(0)
label_distribution.plot(kind='bar', stacked=True, figsize=(12, 6))
plt.title('Label Distribution by Subreddit')
plt.xlabel('Subreddit')
plt.ylabel('Count of Labels')
plt.show()



# Count the number of posts per subreddit
plt.figure(figsize=(12, 6))
df['subreddit'].value_counts().plot(kind='bar')
plt.title('Number of Posts per Subreddit')
plt.xlabel('Subreddit')
plt.ylabel('Count of Posts')
plt.show()







# Resample the data by month (change this to 'W' for weekly or 'D' for daily)
label_trend = df.resample('M')['label'].value_counts().unstack().fillna(0)

# Plot the trend over time for both labels
plt.figure(figsize=(12, 6))
plt.plot(label_trend.index, label_trend[0], label='Non Depressive', color='blue', marker='o')
plt.plot(label_trend.index, label_trend[1], label='Depressive', color='green', marker='o')

# Adding titles and labels
plt.title('Trend Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Posts')
plt.legend(title='Labels')
plt.grid(True)

# Show the plot
plt.show()


# Stacked area plot for label trends over time
plt.figure(figsize=(12, 6))
plt.stackplot(label_trend.index, label_trend[0], label_trend[1], labels=['Non Depressive', 'Depressive'], colors=['skyblue', 'lightgreen'])

# Adding titles and labels
plt.title('Distribution Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Posts')
plt.legend(title='Labels')
plt.grid(True)

# Show the plot
plt.show()


# Correlation heatmap between numeric variables
plt.figure(figsize=(8,6))
sns.heatmap(df[['upvotes', 'num_comments', 'label']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()





# Convert the 'subreddit' column to numeric using label encoding
df['subreddit_encoded'] = LabelEncoder().fit_transform(df['subreddit'])

# including the encoded 'subreddit' in the heatmap
plt.figure(figsize=(8,6))
sns.heatmap(df[['subreddit_encoded', 'upvotes', 'num_comments', 'label']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap with Encoded Subreddit')
plt.show()






#import praw

# Creating a praw instance
#reddit = praw.Reddit(
    #client_id="xxx",
    #client_secret="xxx",
    #user_agent="test_script by /u/xxx",
    #username="xxx",
    #password="xxx"
#)

# Test the connection
#subreddit = reddit.subreddit("xxx")
#for post in subreddit.hot(limit=5):
 #   print(post.title)





#fill in missing values in body
df['body'] = df['body'].fillna('')

# Count words
df['word_count'] = df['body'].apply(lambda x: len(x.split()))

df['char_count'] = df['body'].apply(lambda x: len(x))

df['avg_word_length'] = df['body'].apply(lambda x: np.mean([len(word) for word in x.split()]))

df['title_word_count'] = df['title'].apply(lambda x: len(x.split()))

# Extract hour and day of week
#df['hour'] = df['created_utc'].dt.hour
#df['day_of_week'] = df['created_utc'].dt.dayofweek



#identify upvote to comment ratio
df['upvote_comment_ratio'] = df['upvotes'] / (df['num_comments'] + 1)


df.head(10)





# Group by subreddit and calculate the mean ratio
subreddit_ratio = df.groupby('subreddit')['upvote_comment_ratio'].mean().reset_index()

# Plot the bar chart
plt.figure(figsize=(12,6))
sns.barplot(x='subreddit', y='upvote_comment_ratio', data=subreddit_ratio, palette='viridis')
plt.title('Average Upvote-to-Comment Ratio by Subreddit')
plt.xlabel('Subreddit')
plt.ylabel('Upvote/Comment Ratio')
plt.xticks(rotation=90)
plt.show()


# Create a day of the week column
df['day_of_week'] = df['created_utc'].dt.dayofweek

# Pivot the data to get the mean ratio per subreddit per day
ratio_pivot = df.pivot_table(index='subreddit', columns='day_of_week', values='upvote_comment_ratio', aggfunc='mean')

# Plot the heatmap
plt.figure(figsize=(12,8))
sns.heatmap(ratio_pivot, cmap='YlGnBu', annot=True, fmt=".2f")
plt.title('Upvote-to-Comment Ratio by Subreddit and Day of the Week')
plt.xlabel('Day of the Week')
plt.ylabel('Subreddit')
plt.show()






# Ensure you have the necessary nltk data downloaded
# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('wordnet')

# Define a class TextPreprocessor that conforms to scikit-learnâ€™s transformer API
class TextPreprocessor(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))  # Initialize stop words
        self.lemmatizer = WordNetLemmatizer()  # Initialize lemmatizer

    def fit(self, X, y=None):
        return self  # No fitting required for this transformer

    def transform(self, X, y=None):
        return X.apply(self.preprocess_text)  # Apply preprocessing to each row

    def preprocess_text(self, text):
        if pd.isnull(text):
            return ''  # Return empty string for missing values
        text = text.lower()  # Lowercase
        text = ''.join([char for char in text if char not in string.punctuation])  # Remove punctuation
        tokens = word_tokenize(text)  # Tokenize
        tokens = [word for word in tokens if word not in self.stop_words]  # Remove stopwords
        tokens = [self.lemmatizer.lemmatize(word) for word in tokens]  # Lemmatize
        return ' '.join(tokens)  # Join tokens back into string

# Instantiate TextPreprocessor
text_preprocessor = TextPreprocessor()

# Apply the text preprocessor to 'title' and 'body' columns
df['cleaned_title'] = text_preprocessor.transform(df['title'])
df['cleaned_body'] = text_preprocessor.transform(df['body'])

# Display the processed data
df[['cleaned_title', 'cleaned_body']]



# Combine the cleaned title and body as both may be important
df['combined_text'] = df['cleaned_title'] + ' ' + df['cleaned_body']


# Initialize TF-IDF vectorizer
vectorizer = TfidfVectorizer(max_features=1000)  # You can adjust max_features

# Fit and transform the combined text
X = vectorizer.fit_transform(df['combined_text'])
y = df['label']  # Assuming 'label' is 1 for depression/suicidewatch posts and 0 for others


#review Bag of words for common terms 
#vectorizer = CountVectorizer()
#X_bow = vectorizer.fit_transform(df['body'])





#Set up Logistic regression model
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Logistic Regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print(classification_report(y_test, y_pred))





#naivebayes

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Multinomial Naive Bayes model
nb_model = MultinomialNB()

# Train the model
nb_model.fit(X_train, y_train)

# Predict on test data
y_pred = nb_model.predict(X_test)

# Evaluate the model
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(classification_report(y_test, y_pred))






# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the SVM model
svm_model = SVC(kernel='linear', C=1)  # Start with linear kernel

# Train the model
svm_model.fit(X_train, y_train)

# Predict on test data
y_pred_svm = svm_model.predict(X_test)

# Evaluate the model
print(f"SVM Accuracy: {accuracy_score(y_test, y_pred_svm)}")
print(classification_report(y_test, y_pred_svm))





# Check class balance
print(df['label'].value_counts())

# Check percentage distribution of classes
print(df['label'].value_counts(normalize=True) * 100)

# Check ratio of majority to minority class
class_counts = df['label'].value_counts()
imbalance_ratio = class_counts.max() / class_counts.min()
print(f'Class Imbalance Ratio: {imbalance_ratio:.2f}')

# Plot a pie chart of the class distribution
df['label'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, cmap='coolwarm')
plt.title('Class Distribution in Label Column')
plt.ylabel('')  # Remove y-axis label
plt.show()





# Use SMOTE to oversample the minority class
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)





# Define the parameters you want to tune
param_grid_lr = {
    'C': [0.01, 0.1, 1, 10],  # Regularization parameter
    'solver': ['lbfgs', 'liblinear']  # Different solvers for logistic regression
}

# Initialize Grid Search with 5-fold cross-validation
grid_search_lr = GridSearchCV(LogisticRegression(max_iter=1000), param_grid_lr, cv=5, scoring='accuracy')

# Fit the grid search to the data
grid_search_lr.fit(X_train, y_train)

# Best parameters and best score
print("Best Parameters for Logistic Regression:", grid_search_lr.best_params_)
print("Best Cross-validation Accuracy for Logistic Regression:", grid_search_lr.best_score_)

# Evaluate the best model on the test set
best_lr_model = grid_search_lr.best_estimator_
y_pred_best_lr = best_lr_model.predict(X_test)
print("Test Accuracy with Best Logistic Regression Model:", accuracy_score(y_test, y_pred_best_lr))






# Define the parameters
param_grid = {
    'alpha': [0.1, 0.5, 1.0]  # Add more values if necessary
}

# Initialize Grid Search with 5-fold cross-validation
grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5, scoring='accuracy')

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Best parameters and best score
print("Best Parameters:", grid_search.best_params_)
print("Best Cross-validation Accuracy:", grid_search.best_score_)

# Evaluate the best model on the test set
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test)
print("Test Accuracy with Best Model:", accuracy_score(y_test, y_pred_best))





# Define the parameters you want to tune
param_grid_svm = {
    'C': [0.1, 1, 10],  # Regularization parameter
    'kernel': ['linear'],  # Linear, Radial Basis Function, Polynomial
    #'gamma': ['scale', 'auto'],  # Only matters for 'rbf' or 'poly' kernels
}

# Initialize Grid Search with 5-fold cross-validation
grid_search_svm = GridSearchCV(SVC(), param_grid_svm, cv=5, scoring='accuracy')

# Fit the grid search to the data
grid_search_svm.fit(X_train, y_train)

# Best parameters and best score
print("Best Parameters for SVM:", grid_search_svm.best_params_)
print("Best Cross-validation Accuracy for SVM:", grid_search_svm.best_score_)

# Evaluate the best model on the test set
best_svm_model = grid_search_svm.best_estimator_
y_pred_best_svm = best_svm_model.predict(X_test)
print("Test Accuracy with Best SVM Model:", accuracy_score(y_test, y_pred_best_svm))
print(classification_report(y_test, y_pred_best_svm))






# Initialize the VADER sentiment analyzer
sid = SentimentIntensityAnalyzer()

# Apply VADER to calculate sentiment scores
df['vader_sentiment'] = df['body'].apply(lambda x: sid.polarity_scores(x)['compound'])

# Classify sentiment based on compound score
df['predicted_sentiment'] = df['vader_sentiment'].apply(lambda x: 1 if x > 0 else 0)


df.head(5)


df.head (5)






# Plot the counts of each category
plt.figure(figsize=(8,6))
sns.countplot(x='vader_sentiment', data=df, palette='coolwarm')
plt.title('Distribution of Sentiment Categories')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()



# Group by date and calculate average sentiment
df_daily_sentiment = df.groupby(df['date'].dt.date)['compound'].mean()

# Plot the trend
plt.figure(figsize=(12,6))
plt.plot(df_daily_sentiment.index, df_daily_sentiment.values, marker='o', linestyle='-')
plt.title('Daily Sentiment Trend')
plt.xlabel('Date')
plt.ylabel('Average Sentiment Score')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()



# Generate word clouds for positive and negative sentiment
positive_text = ' '.join(df[df['compound'] >= 0.05]['cleaned_body'])
negative_text = ' '.join(df[df['compound'] <= -0.05]['cleaned_body'])

wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(positive_text)
wordcloud_negative = WordCloud(width=800, height=400, background_color='black').generate(negative_text)

# Plot word clouds
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.imshow(wordcloud_positive, interpolation='bilinear')
plt.axis('off')
plt.title('Positive Sentiment Word Cloud')

plt.subplot(1, 2, 2)
plt.imshow(wordcloud_negative, interpolation='bilinear')
plt.axis('off')
plt.title('Negative Sentiment Word Cloud')

plt.show()



#Boxplot of sentiment by subreddit
plt.figure(figsize=(12,6))
sns.boxplot(x='subreddit', y='compound', data=df, palette='coolwarm')
plt.title('Sentiment Scores by Subreddit')
plt.xlabel('Subreddit')
plt.ylabel('Sentiment Score')
plt.xticks(rotation=90)
plt.show()


#violinplot of sentiment by subreddit
plt.figure(figsize=(12,6))
sns.violinplot(x='subreddit', y='compound', data=df, palette='coolwarm')
plt.title('Sentiment Distribution Across Subreddits (Violin Plot)')
plt.xlabel('Subreddit')
plt.ylabel('Sentiment Score')
plt.xticks(rotation=90)
plt.show()

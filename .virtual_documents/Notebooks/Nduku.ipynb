


# Import necessary libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from datetime import datetime




df= pd.read_csv("C:/Users/User/OneDrive/Documents/Flatiron/Coursework/Phase 5 Capstone/FinalProject/final_project/Data/reduced_reddit.csv")
df.info()
df.isnull().sum()


df.head(5)


df.describe()



# Count unique subreddits
unique_subreddits = df['subreddit'].nunique()
print(f"Number of unique subreddits: {unique_subreddits}")

# List unique subreddits
subreddit_categories = df['subreddit'].unique()
print(f"Subreddit categories: {subreddit_categories}")



# Convert 'created_utc' from float to readable datetime
df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')


# Analyze label distribution by subreddit
label_analysis = df.groupby('subreddit')['label'].value_counts().unstack(fill_value=0)

# Show the analysis of labels by subreddit
print(label_analysis)


df.head(5)






# Histogram of upvotes
plt.figure(figsize=(10,6))
sns.histplot(df['upvotes'], kde=True, bins=20)
plt.title('Distribution of Upvotes')
plt.xlabel('Upvotes')
plt.ylabel('Frequency')
plt.show()

# Histogram of num_comments
plt.figure(figsize=(10,6))
sns.histplot(df['num_comments'], kde=True, bins=20)
plt.title('Distribution of Number of Comments')
plt.xlabel('Number of Comments')
plt.ylabel('Frequency')
plt.show()



# Bar plot for label distribution by subreddit
label_distribution = df.groupby('subreddit')['label'].value_counts().unstack().fillna(0)
label_distribution.plot(kind='bar', stacked=True, figsize=(12, 6))
plt.title('Label Distribution by Subreddit')
plt.xlabel('Subreddit')
plt.ylabel('Count of Labels')
plt.show()



# Count the number of posts per subreddit
plt.figure(figsize=(12, 6))
df['subreddit'].value_counts().plot(kind='bar')
plt.title('Number of Posts per Subreddit')
plt.xlabel('Subreddit')
plt.ylabel('Count of Posts')
plt.show()



# Plot number of posts over time
df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')
df.set_index('created_utc', inplace=True)

# Plot number of posts per month
df.resample('M').size().plot(figsize=(12, 6))
plt.title('Number of Posts Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Posts')
plt.show()




# Resample the data by month (change this to 'W' for weekly or 'D' for daily)
label_trend = df.resample('M')['label'].value_counts().unstack().fillna(0)

# Plot the trend over time for both labels
plt.figure(figsize=(12, 6))
plt.plot(label_trend.index, label_trend[0], label='Non Depressive', color='blue', marker='o')
plt.plot(label_trend.index, label_trend[1], label='Depressive', color='green', marker='o')

# Adding titles and labels
plt.title('Trend Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Posts')
plt.legend(title='Labels')
plt.grid(True)

# Show the plot
plt.show()


# Stacked area plot for label trends over time
plt.figure(figsize=(12, 6))
plt.stackplot(label_trend.index, label_trend[0], label_trend[1], labels=['Non Depressive', 'Depressive'], colors=['skyblue', 'lightgreen'])

# Adding titles and labels
plt.title('Distribution Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Posts')
plt.legend(title='Labels')
plt.grid(True)

# Show the plot
plt.show()


# Correlation heatmap between numeric variables
plt.figure(figsize=(8,6))
sns.heatmap(df[['upvotes', 'num_comments', 'label']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()





# Convert the 'subreddit' column to numeric using label encoding
df['subreddit_encoded'] = LabelEncoder().fit_transform(df['subreddit'])

# including the encoded 'subreddit' in the heatmap
plt.figure(figsize=(8,6))
sns.heatmap(df[['subreddit_encoded', 'upvotes', 'num_comments', 'label']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap with Encoded Subreddit')
plt.show()






#import praw

# Creating a praw instance
#reddit = praw.Reddit(
    #client_id="xxx",
    #client_secret="xxx",
    #user_agent="test_script by /u/xxx",
    #username="xxx",
    #password="xxx"
#)

# Test the connection
#subreddit = reddit.subreddit("xxx")
#for post in subreddit.hot(limit=5):
 #   print(post.title)





#fill in missing values in body
df['body'] = df['body'].fillna('')

# Count words
df['word_count'] = df['body'].apply(lambda x: len(x.split()))

df['char_count'] = df['body'].apply(lambda x: len(x))

df['avg_word_length'] = df['body'].apply(lambda x: np.mean([len(word) for word in x.split()]))

df['title_word_count'] = df['title'].apply(lambda x: len(x.split()))



# Extract hour and day of week
df['hour'] = df['created_utc'].dt.hour
df['day_of_week'] = df['created_utc'].dt.dayofweek


#identify upvote to comment ratio
df['upvote_comment_ratio'] = df['upvotes'] / (df['num_comments'] + 1)


df.head(10)






# Define function
def preprocess_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove non-alphabetic characters
    tokens = word_tokenize(text)  # Tokenize
    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords
    return ' '.join(tokens)  # Rejoin tokens into a single string

# Apply the function to both titles and bodies
df['cleaned_title'] = df['title'].apply(preprocess_text)
df['cleaned_body'] = df['body'].apply(preprocess_text)


# Combine the cleaned title and body as both may be important
df['combined_text'] = df['cleaned_title'] + ' ' + df['cleaned_body']


# Initialize TF-IDF vectorizer
vectorizer = TfidfVectorizer(max_features=1000)  # You can adjust max_features

# Fit and transform the combined text
X = vectorizer.fit_transform(df['combined_text'])
y = df['label']  # Assuming 'label' is 1 for depression/suicidewatch posts and 0 for others


#review Bag of words for common terms 
#vectorizer = CountVectorizer()
#X_bow = vectorizer.fit_transform(df['body'])


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train the Logistic Regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print(classification_report(y_test, y_pred))





# Check class balance
print(df['label'].value_counts())

# Check percentage distribution of classes
print(df['label'].value_counts(normalize=True) * 100)

# Check ratio of majority to minority class
class_counts = df['label'].value_counts()
imbalance_ratio = class_counts.max() / class_counts.min()
print(f'Class Imbalance Ratio: {imbalance_ratio:.2f}')

# Plot a pie chart of the class distribution
df['label'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, cmap='coolwarm')
plt.title('Class Distribution in Label Column')
plt.ylabel('')  # Remove y-axis label
plt.show()





# Use SMOTE to oversample the minority class
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)






# Initialize the VADER sentiment analyzer
sid = SentimentIntensityAnalyzer()

# Apply VADER to calculate sentiment scores
df['vader_sentiment'] = df['body'].apply(lambda x: sid.polarity_scores(x)['compound'])

# Classify sentiment based on compound score
df['predicted_sentiment'] = df['vader_sentiment'].apply(lambda x: 1 if x > 0 else 0)


df.head(5)





# Combine the cleaned title and body
df['combined_text'] = df['title'] + ' ' + df['body']


df.head (5)




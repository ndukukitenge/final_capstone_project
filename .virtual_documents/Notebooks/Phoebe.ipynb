





#import libraries needed
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA


from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier


#Load the dataset to use 
#data = pd.read_csv(file_path)  #
df = pd.read_csv("C:\\Users\\User\\OneDrive\\Desktop\\Flatiron\\Phase_five_final_06_11_2024\\final_project\\Data\\reduced_reddit.csv", index_col=False)


df.head(5)


#check the dataset information 
df.info()



#check on the description of numerical datatypes
df.describe()





#check to see if there are null values in percentage form
df.isnull().sum() / len(df) * 100








# Now you can fill missing values in 'body' column for the new 'data_main'
df['num_comments'] = df['num_comments'].fillna(0)

# Check the DataFrame info again to confirm the change
df.info() 


df.isnull().sum()


# We fill the missing values in 'body' column for the new 'data_main'
df['body'] = df['body'].fillna("No content")

# Check the DataFrame info again to confirm the change
df.info() 


# Now you can fill missing values in 'body' column for the new 'data_main'
df['body'] = df['body'].fillna("No content")

# Check the DataFrame info again to confirm the change
df.info() 


#check for missing values 
df.isnull().sum()


# Convert 'created_utc' column from UTC epoch time to datetime
df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')

# To see the updated DataFrame

df





#Visualize the frequency distribution of values using histograms and to identify outliers using the box plots- for upvotes and num_comments

sns.set(style="whitegrid")
numeric_columns = ['upvotes', 'num_comments', 'label']

# Set up the subplot grid
fig, axes = plt.subplots(1, len(numeric_columns), figsize=(16, 5))

for i, col in enumerate(numeric_columns):
    sns.histplot(df[col], bins=5, kde=True, ax=axes[i], color='skyblue')
    axes[i].set_title(f'Histogram of {col}')
    axes[i].set_xlabel(col.capitalize())
    axes[i].set_ylabel("Frequency")

plt.tight_layout()
plt.show()


# Upvotes box plot
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 2)
df['upvotes'].plot(kind='box')
plt.title('Upvotes Box Plot')
plt.show()

#for num_comments
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 2)
df['num_comments'].plot(kind='box')
plt.title('Num Comments Box Plot')
plt.show()


#Frequency of categorical columns

# Frequency of 'subreddit' column
print(df['subreddit'].value_counts())

# Frequency of 'label' column
print(df['label'].value_counts())

# Set style for plots
sns.set(style="whitegrid")

# Plot the counts for 'subreddit' column
plt.figure(figsize=(12, 6))
sns.countplot(data=df, y='subreddit', order=df['subreddit'].value_counts().index, palette='viridis')
plt.title('Counts of Posts by Subreddit')
plt.xlabel('Count')
plt.ylabel('Subreddit')
plt.show()

# Plot the counts for 'label' column
plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='label', palette='coolwarm')
plt.title('Counts by Label')
plt.xlabel('Label')
plt.ylabel('Count')
plt.xticks(ticks=[0, 1], labels=['Non-suicidal', 'Suicidal'])


# Plot number of posts over time
df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')
df.set_index('created_utc', inplace=True)

# Plot number of posts per month
df.resample('M').size().plot(figsize=(12, 6))
plt.title('Number of Posts Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Posts')
plt.show()


# Resample the data by month (change this to 'W' for weekly or 'D' for daily)
label_trend = df.resample('M')['label'].value_counts().unstack().fillna(0)

# Plot the trend over time for both labels
plt.figure(figsize=(12, 6))
plt.plot(label_trend.index, label_trend[0], label='Label 0', color='blue', marker='o')
plt.plot(label_trend.index, label_trend[1], label='Label 1', color='green', marker='o')

# Adding titles and labels
plt.title('Trend of Labels 0 and 1 Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Posts')
plt.legend(title='Labels')
plt.grid(True)

# Show the plot
plt.show()





#Numerical vs. Numerical (Scatter plot and correlation matrix)
#Scatter Plot - Shows the relationship between two numerical variables, upvotes and num_comments.

# Scatter plot for numerical variables (Upvotes vs. Number of Comments)
plt.figure(figsize=(8, 6))
sns.scatterplot(x='upvotes', y='num_comments', data=df, alpha=0.5, color='orange')
plt.title("Scatter Plot of Upvotes vs. Number of Comments")
plt.xlabel("Upvotes")
plt.ylabel("Number of Comments")
plt.show()

# Correlation matrix and heatmap
correlation_matrix = df[['upvotes', 'num_comments']].corr()

# Plot the correlation matrix as a heatmap
plt.figure(figsize=(6, 5))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True, linewidths=0.5, vmin=-1, vmax=1)
plt.title("Correlation Matrix of Numerical Variables")
plt.show()


# Numerical vs. Categorical (Box plot and Violin plot)
# Box plot of 'upvotes' by 'label'

plt.figure(figsize=(8, 6))
sns.boxplot(x='label', y='upvotes', data=df, palette='pastel')
plt.title("Box Plot of Upvotes by Label", fontsize=16)
plt.xlabel("Label", fontsize=14)
plt.ylabel("Upvotes", fontsize=14)
plt.xticks(ticks=[0, 1], labels=["Non-Suicidal (0)", "Suicidal (1)"])
plt.show()

# Violin plot of 'num_comments' by 'subreddit'
plt.figure(figsize=(12, 6))
sns.violinplot(x='subreddit', y='num_comments', data=df, palette='muted')
plt.title("Violin Plot of Number of Comments by Subreddit", fontsize=16)
plt.xlabel("Subreddit", fontsize=14)
plt.ylabel("Number of Comments", fontsize=14)
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.show()


# Crosstab between 'subreddit' and 'label'

crosstab = pd.crosstab(df['subreddit'], df['label'])
print("Crosstab of Subreddit by Label:")
print(crosstab)

# Bar plot for 'subreddit' and 'label'
plt.figure(figsize=(12, 6))
sns.countplot(x="subreddit", hue="label", data=df, palette="Set2")  # Correct 'data=df'
plt.title("Subreddit Distribution by Label", fontsize=16)
plt.xlabel("Subreddit", fontsize=14)
plt.ylabel("Count", fontsize=14)
plt.legend(title="Label", loc='upper right', labels=["Non-Suicidal (0)", "Suicidal (1)"])
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.show()





df.columns  # Check all column names


# Convert 'created_utc' to Unix timestamp (numerical format)
df['created_utc'] = df['created_utc'].astype(int) / 10**9  # Convert to seconds

# Pair plot with 'label' as the hue, including 'created_utc'
sns.pairplot(df[['upvotes', 'num_comments', 'created_utc', 'label']], hue='label', palette='Set1', diag_kind='kde')
plt.suptitle("Pair Plot of Numerical Variables by Label", y=1.02)
plt.show()


# Convert the 'subreddit' column to numeric using label encoding
df['subreddit_encoded'] = LabelEncoder().fit_transform(df['subreddit'])

# Now include the encoded 'subreddit' in the heatmap
plt.figure(figsize=(8,6))
sns.heatmap(df[['subreddit_encoded', 'upvotes', 'num_comments', 'label']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap with Encoded Subreddit')
plt.show()


# One-hot encoding the 'subreddit' column
df_encoded = pd.get_dummies(df, columns=['subreddit'], drop_first=True)

# Check the first few rows of the encoded DataFrame
print(df_encoded.head())

# Now, let's create the correlation heatmap again with the one-hot encoded 'subreddit' columns
plt.figure(figsize=(10, 6))
sns.heatmap(df_encoded[['subreddit_teenagers', 'subreddit_depression', 'subreddit_SuicideWatch', 'upvotes', 'num_comments', 'label']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap with One-Hot Encoded Subreddit')
plt.show()


# --- 1. Feature Creation ---
# Convert 'created_utc' to datetime format
df['created_utc'] = pd.to_datetime(df['created_utc'])

# Extract year, month, day, hour from 'created_utc'
df['year'] = df['created_utc'].dt.year
df['month'] = df['created_utc'].dt.month
df['day'] = df['created_utc'].dt.day
df['hour'] = df['created_utc'].dt.hour

# Create additional features like ratios or flags
df['upvotes_per_comment'] = df['upvotes'] / (df['num_comments'] + 1)  # Adding 1 to avoid division by zero
df['has_body'] = df['body'].apply(lambda x: 0 if x == 'No content' else 1)  # Flag indicating if there's content in 'body'

# --- 2. Feature Encoding ---
# Encode the 'subreddit' categorical feature using one-hot encoding
df = pd.get_dummies(df, columns=['subreddit'], drop_first=True)

# --- 3. Scaling and Normalization ---
# Initialize scalers
standard_scaler = StandardScaler()
minmax_scaler = MinMaxScaler()

# Apply Standard Scaling on numerical features
df[['upvotes', 'num_comments', 'upvotes_per_comment']] = standard_scaler.fit_transform(
    df[['upvotes', 'num_comments', 'upvotes_per_comment']]
)

# Apply Min-Max Scaling on year, month, day, hour (optional)
df[['year', 'month', 'day', 'hour']] = minmax_scaler.fit_transform(
    df[['year', 'month', 'day', 'hour']]
)

# Display the final DataFrame
print(df)





import pandas as pd
import numpy as np
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_selection import chi2

# Step 1: Load the dataset
df = pd.read_csv("C:\\Users\\User\\OneDrive\\Desktop\\Flatiron\\Phase_five_final_06_11_2024\\final_project\\Data\\reduced_reddit.csv", index_col=False)

# Step 2: Combine 'title' and 'body' columns for the full text
df['full_text'] = df['title'].fillna('') + " " + df['body'].fillna('')

# Step 3: Preprocess the text (remove non-alphabet characters, lowercase)
def preprocess_text(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text.lower())
    return text

df['processed_text'] = df['full_text'].apply(preprocess_text)

# Step 4: Vectorize the text using CountVectorizer
vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(df['processed_text'])

# Step 5: Create labels (assuming 'label' is 0 for non-depressive and 1 for depressive posts)
y = df['label']

# Step 6: Apply chi-squared test to identify significant words
chi2_stat, p_val = chi2(X, y)

# Step 7: Get the feature names (words) and create a DataFrame with their p-values
words = np.array(vectorizer.get_feature_names())  # Use get_feature_names() instead of get_feature_names_out()
result = pd.DataFrame({'Word': words, 'Chi2 Stat': chi2_stat, 'P-Value': p_val})

# Step 8: Sort by p-value to identify most significant words
result = result.sort_values('P-Value', ascending=True)

# Step 9: Output the most significant words
print(result.head(20))  # Shows the top 20 most significant words

# Optionally, filter based on p-value threshold
significant_words = result[result['P-Value'] < 0.05]
print("Most significant words:")
print(significant_words)



# Check for Missing Values ---
print(df.isnull().sum())  # Ensure no missing values before proceeding

# Display the final DataFrame
print(df.head())  # Show the first few rows to check the result





import pandas as pd
import numpy as np
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from nltk.corpus import stopwords
import string

# Download NLTK stopwords
nltk.download('stopwords')

# --- Preprocess Text Data ---
def preprocess_text(text):
    """Function to preprocess text by lowercasing, removing punctuation, and stopwords."""
    text = text.lower()  # Convert to lowercase
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    stop_words = set(stopwords.words('english'))  # Set of stopwords from NLTK
    text = " ".join([word for word in text.split() if word not in stop_words])  # Remove stopwords
    return text

# --- Text Preprocessing on 'title' and 'body' columns ---
df['processed_title'] = df['title'].apply(preprocess_text)
df['processed_body'] = df['body'].apply(preprocess_text)

# --- Combine Title and Body for Text Analysis ---
df['processed_text'] = df['processed_title'] + " " + df['processed_body']

# --- Define Features and Labels ---
X = df['processed_text']  # Features: text data
y = df['label']  # Labels: sentiment labels (0 = negative, 1 = positive, 2 = neutral)

# Split the dataset into training and test sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- Build the Pipeline ---
# We will use TF-IDF Vectorizer and Logistic Regression together in a pipeline
model = make_pipeline(
    TfidfVectorizer(max_features=5000),  # Convert text data into numerical form (max 5000 features)
    LogisticRegression(max_iter=1000)    # Train a Logistic Regression classifier
)

# --- Train the Model ---
model.fit(X_train, y_train)

# --- Make Predictions ---
y_pred = model.predict(X_test)

# --- Evaluate the Model ---
print("Accuracy:", accuracy_score(y_test, y_pred))  # Overall accuracy
print("\nClassification Report:\n", classification_report(y_test, y_pred))  # Precision, Recall, F1-Score









# Download NLTK stopwords
nltk.download('stopwords')

# --- Preprocess Text Data ---
def preprocess_text(text):
    """Function to preprocess text by lowercasing, removing punctuation, and stopwords."""
    text = text.lower()  # Convert to lowercase
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    stop_words = set(stopwords.words('english'))  # Set of stopwords from NLTK
    text = " ".join([word for word in text.split() if word not in stop_words])  # Remove stopwords
    return text

# --- Text Preprocessing on 'title' and 'body' columns ---
df['processed_title'] = df['title'].apply(preprocess_text)
df['processed_body'] = df['body'].apply(preprocess_text)

# --- Combine Title and Body for Text Analysis ---
df['processed_text'] = df['processed_title'] + " " + df['processed_body']

# --- Define Features and Labels ---
X = df['processed_text']  # Features: text data
y = df['label']  # Labels: sentiment labels (0 = negative, 1 = positive, 2 = neutral)

# Split the dataset into training and test sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- Build the Pipeline ---
# We will use TF-IDF Vectorizer and Multinomial Naive Bayes together in a pipeline
model = make_pipeline(
    TfidfVectorizer(max_features=5000),  # Convert text data into numerical form (max 5000 features)
    MultinomialNB()                      # Train a Multinomial Naive Bayes classifier
)

# --- Train the Model ---
model.fit(X_train, y_train)

# --- Make Predictions ---
y_pred = model.predict(X_test)

# --- Evaluate the Model ---
print("Accuracy:", accuracy_score(y_test, y_pred))  # Overall accuracy
print("\nClassification Report:\n", classification_report(y_test, y_pred))  # Precision, Recall, F1-Score





# Download NLTK stopwords
nltk.download('stopwords')
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
# --- Preprocess Text Data ---
def preprocess_text(text):
    """Function to preprocess text by lowercasing, removing punctuation, and stopwords."""
    text = text.lower()  # Convert to lowercase
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    stop_words = set(stopwords.words('english'))  # Set of stopwords from NLTK
    text = " ".join([word for word in text.split() if word not in stop_words])  # Remove stopwords
    return text

# --- Text Preprocessing on 'title' and 'body' columns ---
df['processed_title'] = df['title'].apply(preprocess_text)
df['processed_body'] = df['body'].apply(preprocess_text)

# --- Combine Title and Body for Text Analysis ---
df['processed_text'] = df['processed_title'] + " " + df['processed_body']

# --- Define Features and Labels ---
X = df['processed_text']  # Features: text data
y = df['label']  # Labels: sentiment labels (0 = negative, 1 = positive, 2 = neutral)

# Split the dataset into training and test sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- Build the Pipeline ---
# We will use TF-IDF Vectorizer and Support Vector Machine (SVM) together in a pipeline
model = make_pipeline(
    TfidfVectorizer(max_features=5000),  # Convert text data into numerical form (max 5000 features)
    LinearSVC()                          # Train a Linear Support Vector Classifier (SVM)
)

# --- Train the Model ---
model.fit(X_train, y_train)

# --- Make Predictions ---
y_pred = model.predict(X_test)

# --- Evaluate the Model ---
print("Accuracy:", accuracy_score(y_test, y_pred))  # Overall accuracy
print("\nClassification Report:\n", classification_report(y_test, y_pred))  # Precision, Recall, F1-Score





import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()

# Stopwords list
stop_words = set(stopwords.words('english'))

# Preprocessing function with lemmatization
def preprocess_text(text):
    text = text.lower()  # Convert to lowercase
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    text = " ".join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])  # Remove stopwords and lemmatize
    return text

# Apply preprocessing to 'title' and 'body' columns
df['processed_title'] = df['title'].apply(preprocess_text)
df['processed_body'] = df['body'].apply(preprocess_text)

# Combine processed title and body
df['processed_text'] = df['processed_title'] + " " + df['processed_body']





from sklearn.feature_extraction.text import TfidfVectorizer

# Vectorize the 'processed_text' column using TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust the number of features

# Fit and transform the training data
X_tfidf = tfidf_vectorizer.fit_transform(df['processed_text'])

# Show the shape of the resulting matrix (number of documents, number of features)
print(X_tfidf.shape)








from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['label'], test_size=0.2, random_state=42)

# Initialize the Logistic Regression model
logreg = LogisticRegression()

# Train the model
logreg.fit(X_train, y_train)

# Make predictions on the test set
y_pred = logreg.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))





from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2'], yticklabels=['Class 0', 'Class 1', 'Class 2'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()






from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['label'], test_size=0.2, random_state=42)

# Initialize the Naive Bayes model
nb = MultinomialNB()

# Train the model
nb.fit(X_train, y_train)

# Make predictions on the test set
y_pred = nb.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))






from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['label'], test_size=0.2, random_state=42)

# Initialize the SVM model
svm_model = SVC(kernel='linear')  # Using a linear kernel for text classification

# Train the model
svm_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = svm_model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))






from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['label'], test_size=0.2, random_state=42)

# Initialize the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))






from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['label'], test_size=0.2, random_state=42)

# Initialize the KNN model
knn_model = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = knn_model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))






from sklearn.model_selection import train_test_split

# Define features and target
X = df.drop(columns=['label'])  # Features
y = df['label']  # Target

# Split into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training Set Shape:", X_train.shape)
print("Test Set Shape:", X_test.shape)





from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

# Step 1: Create the TF-IDF vectorizer for 'title' and 'body' columns
vectorizer_title = TfidfVectorizer(max_features=1000)  # Limit to 1000 features for simplicity
vectorizer_body = TfidfVectorizer(max_features=1000)

# Step 2: Convert 'title' and 'body' columns into numeric features using TF-IDF
X_title = vectorizer_title.fit_transform(df['title'])
X_body = vectorizer_body.fit_transform(df['body'])

# Step 3: Concatenate the resulting features with the numeric features (e.g., 'upvotes', 'num_comments')
from scipy.sparse import hstack
X = hstack([X_title, X_body, df[['upvotes', 'num_comments']].values])

# Step 4: Set the target variable ('label')
y = df['label']

# Step 5: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 6: Train the model (Logistic Regression as an example)
model = LogisticRegression(max_iter=1000)  # Ensure the solver converges

# Fit the model on the training data
model.fit(X_train, y_train)

# Step 7: Make predictions on the test set
y_pred = model.predict(X_test)

# Step 8: Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))








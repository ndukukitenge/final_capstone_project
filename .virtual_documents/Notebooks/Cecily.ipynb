import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud

import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

import string
import demoji  



#data = pd.read_csv(file_path)  # 
data = pd.read_csv('../Data/reduced_reddit.csv', index_col=False)
print(data.head())  # Display the first few rows of the dataframe



print(data.info())  # Check data types and missing values


print(data.columns)



# Combine 'title' and 'body' into a new column called 'combined_text'
data['combined_text'] = data['title'].fillna('') + ' ' + data['body'].fillna('')

# Display the first few rows to confirm the new column
print(data[['title', 'body', 'combined_text']].head())



data['combined_text'] = data['title'].fillna('') + ' ' + data['body'].fillna('')



# Fill missing values for 'title' and 'body' columns
data['title'] = data['title'].fillna('')
data['body'] = data['body'].fillna('')

# Create 'combined_text' column
data['combined_text'] = data['title'] + ' ' + data['body']

# Convert text to lowercase
data['combined_text'] = data['combined_text'].str.lower()

# Remove punctuation and special characters
import re
data['combined_text'] = data['combined_text'].apply(lambda x: re.sub(r'[^\w\s]', '', x))

# Optional: Remove numbers
data['combined_text'] = data['combined_text'].apply(lambda x: re.sub(r'\d+', '', x))




stop_words = set(stopwords.words('english')).union(set(string.punctuation))
custom_stopwords = {'additional', 'stopwords', 'to', 'add'}  # Converting custom stopwords to a set
stop_words.update(custom_stopwords)



print(data.columns)



from nltk.tokenize import word_tokenize
nltk.download('punkt')  # Download the tokenizer if not already done

# Tokenize 'combined_text' and create 'tokens' column
data['tokens'] = data['combined_text'].apply(lambda x: word_tokenize(x))



print(data[['combined_text', 'tokens']].head())



from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()
# Apply lemmatization to each token in the 'tokens' column
data['tokens'] = data['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])



from nltk.probability import FreqDist

# Flatten tokens list for frequency distribution
all_tokens = [word for tokens in data['tokens'] for word in tokens]
freq_dist = FreqDist(all_tokens)

# Generate word cloud
wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(freq_dist)

# Plot word cloud
plt.figure(figsize=(15, 10))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud')
plt.show()



# Plot top 20 most common words
most_common_words = freq_dist.most_common(20)
words, counts = zip(*most_common_words)
plt.figure(figsize=(10, 6))
plt.bar(words, counts)
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Top 20 Most Common Words')
plt.xticks(rotation=45)
plt.show()



data['word_count'] = data['combined_text'].apply(lambda x: len(x.split()))
plt.figure(figsize=(10, 6))
plt.hist(data['word_count'], bins=30, color='skyblue', edgecolor='black')
plt.xlabel('Word Count')
plt.ylabel('Frequency')
plt.title('Distribution of Word Counts in Combined Text')
plt.show()



from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
import matplotlib.pyplot as plt

vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')
X = vectorizer.fit_transform(data['combined_text'])

# Use the sum of each feature directly from the sparse matrix
bigram_counts = X.sum(axis=0).A1  # A1 converts it to a flat array
bigrams = vectorizer.get_feature_names_out()

# Create a DataFrame with bigrams and their counts
bigram_freq = pd.DataFrame({'bigram': bigrams, 'count': bigram_counts})
bigram_freq = bigram_freq.sort_values(by='count', ascending=False).head(20)

# Plot the top 20 bigrams
plt.figure(figsize=(10, 6))
plt.barh(bigram_freq['bigram'], bigram_freq['count'], color='teal')
plt.xlabel('Count')
plt.ylabel('Bigram')
plt.title('Top 20 Bigrams')
plt.gca().invert_yaxis()
plt.show()



pip install textblob



import nltk
nltk.download('brown')
nltk.download('punkt')



from textblob import TextBlob

# Calculate sentiment polarity
data['sentiment_polarity'] = data['combined_text'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Plot sentiment distribution
plt.figure(figsize=(10, 6))
plt.hist(data['sentiment_polarity'], bins=30, color='purple', edgecolor='black')
plt.xlabel('Sentiment Polarity')
plt.ylabel('Frequency')
plt.title('Distribution of Sentiment Polarity')
plt.show()



# Example: Count unique words per subreddit (or any category)
unique_words = data.groupby('subreddit')['tokens'].apply(lambda x: set([word for tokens in x for word in tokens]))
unique_word_count = unique_words.apply(len)

# Plot
unique_word_count.plot(kind='bar', figsize=(12, 6), color='orange')
plt.xlabel('Subreddit')
plt.ylabel('Number of Unique Words')
plt.title('Unique Words per Subreddit')
plt.xticks(rotation=45)
plt.show()



from wordcloud import WordCloud

# Generate word clouds for a specific category (e.g., subreddit)
for subreddit in data['subreddit'].unique():
    text = ' '.join(data.loc[data['subreddit'] == subreddit, 'combined_text'])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f'Word Cloud for Subreddit: {subreddit}')
    plt.axis('off')
    plt.show()



data['created_date'] = pd.to_datetime(data['created_utc'], unit='s')  # Convert Unix timestamp
data['month_year'] = data['created_date'].dt.to_period('M')

# Plot post counts over time
data.groupby('month_year').size().plot(kind='line', figsize=(12, 6))
plt.xlabel('Month-Year')
plt.ylabel('Number of Posts')
plt.title('Post Frequency Over Time')
plt.xticks(rotation=45)
plt.show()



# Summary statistics
print(data[['upvotes', 'num_comments']].describe())



import seaborn as sns
import matplotlib.pyplot as plt

# Distribution of upvotes
plt.figure(figsize=(10, 5))
sns.histplot(data['upvotes'], bins=30, kde=True, color='blue')
plt.title('Distribution of Upvotes')
plt.xlabel('Upvotes')
plt.ylabel('Frequency')
plt.show()

# Distribution of num_comments
plt.figure(figsize=(10, 5))
sns.histplot(data['num_comments'], bins=30, kde=True, color='green')
plt.title('Distribution of Number of Comments')
plt.xlabel('Number of Comments')
plt.ylabel('Frequency')
plt.show()



# Correlation matrix
correlation_matrix = data[['upvotes', 'num_comments', 'sentiment_polarity']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()



from sklearn.feature_extraction.text import TfidfVectorizer

# Create a TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')  # Adjust max_features based on your dataset size

# Fit and transform the text data
tfidf_matrix = tfidf_vectorizer.fit_transform(data['combined_text'])

# Display the shape of the resulting TF-IDF matrix
print("TF-IDF Matrix Shape:", tfidf_matrix.shape)

# Optionally, you can convert the matrix to a DataFrame for inspection
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())
print(tfidf_df.head())



pip install transformers



pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118



pip install keras==2.11.0



pip install tensorflow==2.11.0



pip install keras==2.11.0



pip install torch torchvision torchaudio



pip uninstall tensorflow tensorflow-intel keras



pip install tensorflow==2.11 keras==2.11


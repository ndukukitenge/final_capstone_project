#import libraries needed
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA


from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier


#Load the dataset to use 
#data = pd.read_csv(file_path)  
data = pd.read_csv('../Data/reduced_reddit.csv', index_col=False)
print(data.head(10))  # Display the first few rows of the dataframe



#check the dataset information 
data.info()


#check on the description of numerical datatypes
data.describe()





#check to see if there are null values in percentage form
data.isnull().sum() / len(data) * 100


# Now you can fill missing values in 'body' column for the new 'data_main'
data['num_comments'] = data['num_comments'].fillna(0)

# Check the DataFrame info again to confirm the change
data.info() 



# Verify that there are no more missing values
data.isnull().sum()


# Now you can fill missing values in 'body' column for the new 'data_main'
data['body'] = data['body'].fillna("No content")

# Check the DataFrame info again to confirm the change
data.info() 


#check for missing values 
data.isnull().sum()





# Remove duplicates and assign it back to the dataframe
data =data.drop_duplicates()
data.info()



# Convert 'created_utc' column from UTC epoch time to datetime
data['created_utc'] = pd.to_datetime(data['created_utc'], unit='s')

# To see the updated DataFrame

data





#Visualize the frequency distribution of values using histograms and to identify outliers using the box plots- for upvotes and num_comments

sns.set(style="whitegrid")
numeric_columns = ['upvotes', 'num_comments', 'label']

# Set up the subplot grid
fig, axes = plt.subplots(1, len(numeric_columns), figsize=(16, 5))

for i, col in enumerate(numeric_columns):
    sns.histplot(data[col], bins=5, kde=True, ax=axes[i], color='skyblue')
    axes[i].set_title(f'Histogram of {col}')
    axes[i].set_xlabel(col.capitalize())
    axes[i].set_ylabel("Frequency")

plt.tight_layout()
plt.show()


# Upvotes box plot
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 2)
data['upvotes'].plot(kind='box')
plt.title('Upvotes Box Plot')
plt.show()

#for num_comments
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 2)
data['num_comments'].plot(kind='box')
plt.title('Num Comments Box Plot')
plt.show()


#Frequency of categorical columns

# Frequency of 'subreddit' column
print(data['subreddit'].value_counts())

# Frequency of 'label' column
print(data['label'].value_counts())

# Set style
sns.set(style="whitegrid")

# Plot the counts for 'subreddit' column
plt.figure(figsize=(12, 6))
sns.countplot(data=data, y='subreddit', order=data['subreddit'].value_counts().index, palette='viridis')
plt.title('Counts of Posts by Subreddit')
plt.xlabel('Count')
plt.ylabel('Subreddit')
plt.show()

# Plot the counts for 'label' column
plt.figure(figsize=(8, 6))
sns.countplot(data=data, x='label', palette='coolwarm')
plt.title('Counts by Label')
plt.xlabel('Label')
plt.ylabel('Count')
plt.xticks(ticks=[0, 1], labels=['Non-suicidal', 'Suicidal'])
plt.show()




# Plot number of posts over time
data['created_utc'] = pd.to_datetime(data['created_utc'], unit='s')
data.set_index('created_utc', inplace=True)

# Plot number of posts per month
data.resample('M').size().plot(figsize=(12, 6))
plt.title('Number of Posts Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Posts')
plt.show()


# Resample the data by month (change this to 'W' for weekly or 'D' for daily)
label_trend = data.resample('M')['label'].value_counts().unstack().fillna(0)

# Plot the trend over time for both labels
plt.figure(figsize=(12, 6))
plt.plot(label_trend.index, label_trend[0], label='Label 0', color='blue', marker='o')
plt.plot(label_trend.index, label_trend[1], label='Label 1', color='green', marker='o')

# Adding titles and labels
plt.title('Trend of Labels 0 and 1 Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Posts')
plt.legend(title='Labels')
plt.grid(True)

# Show the plot
plt.show()





#Numerical vs. Numerical (Scatter plot and correlation matrix)
#Scatter Plot - Shows the relationship between two numerical variables, upvotes and num_comments.

# Scatter plot for numerical variables
plt.figure(figsize=(8, 6))
sns.scatterplot(x='upvotes', y='num_comments', data=data, alpha=0.5)
plt.title("Scatter Plot of Upvotes vs. Number of Comments")
plt.xlabel("Upvotes")
plt.ylabel("Number of Comments")
plt.show()

# Correlation matrix and heatmap
correlation_matrix = data[['upvotes', 'num_comments']].corr()
plt.figure(figsize=(6, 5))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)
plt.title("Correlation Matrix of Numerical Variables")
plt.show()


# Numerical vs. Categorical (Box plot and Violin plot)
# Box plot of 'upvotes' by 'label'
plt.figure(figsize=(8, 6))
sns.boxplot(x='label', y='upvotes', data=data, palette='pastel')
plt.title("Box Plot of Upvotes by Label")
plt.xlabel("Label")
plt.ylabel("Upvotes")
plt.xticks(ticks=[0, 1], labels=["Non-Suicidal (0)", "Suicidal (1)"])
plt.show()

# Violin plot of 'num_comments' by 'subreddit'
plt.figure(figsize=(10, 6))
sns.violinplot(x='subreddit', y='num_comments', data=data, palette='muted')
plt.title("Violin Plot of Number of Comments by Subreddit")
plt.xlabel("Subreddit")
plt.ylabel("Number of Comments")
plt.show()



# Crosstab between 'subreddit' and 'label'
crosstab = pd.crosstab(data['subreddit'], data['label'])
print("Crosstab of Subreddit by Label:")
print(crosstab)

# Bar plot for 'subreddit' and 'label'
plt.figure(figsize=(10, 6))
sns.countplot(data=data, x="subreddit", hue="label", palette="Set2")
plt.title("Subreddit Distribution by Label")
plt.xlabel("Subreddit")
plt.ylabel("Count")
plt.legend(title="Label", loc='upper right', labels=["Non-Suicidal (0)", "Suicidal (1)"])
plt.show()






# Pair plot with 'label' as the hue
sns.pairplot(data[['upvotes', 'num_comments', 'created_utc', 'label']], hue='label', palette='Set1', diag_kind='kde')
plt.suptitle("Pair Plot of Numerical Variables by Label", y=1.02)
plt.show()


# Convert the 'subreddit' column to numeric using label encoding
data['subreddit_encoded'] = LabelEncoder().fit_transform(data['subreddit'])

# Now include the encoded 'subreddit' in the heatmap
plt.figure(figsize=(8,6))
sns.heatmap(data[['subreddit_encoded', 'upvotes', 'num_comments', 'label']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap with Encoded Subreddit')
plt.show()





# --- 1. Feature Creation ---
# Convert 'created_utc' to datetime format
data['created_utc'] = pd.to_datetime(data['created_utc'])

# Extract year, month, day, hour from 'created_utc'
data['year'] = data['created_utc'].dt.year
data['month'] = data['created_utc'].dt.month
data['day'] = data['created_utc'].dt.day
data['hour'] = data['created_utc'].dt.hour

# Create additional features like ratios or flags
data['upvotes_per_comment'] = data['upvotes'] / (data['num_comments'] + 1)  # Adding 1 to avoid division by zero
data['has_body'] = data['body'].apply(lambda x: 0 if x == 'No content' else 1)  # Flag indicating if there's content in 'body'



# Remove the subreddit encoded columns to improve model generalization
data = data.drop(columns=["subreddit_SuicideWatch", "subreddit_depression", "subreddit_happy", "subreddit_teenagers"])


# --- 3. Scaling and Normalization ---
# Initialize scalers
standard_scaler = StandardScaler()
minmax_scaler = MinMaxScaler()

# Apply Standard Scaling on numerical features
data[['upvotes', 'num_comments', 'upvotes_per_comment']] = standard_scaler.fit_transform(
    data[['upvotes', 'num_comments', 'upvotes_per_comment']]
)

# Apply Min-Max Scaling on year, month, day, hour (optional)
data[['year', 'month', 'day', 'hour']] = minmax_scaler.fit_transform(
    data[['year', 'month', 'day', 'hour']]
)

# Display the final DataFrame
data









import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.base import TransformerMixin, BaseEstimator
import string# Download necessary NLTK resources

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

# Initialize stopwords, punctuation, and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

#define a class TextPreprocessor that conforms to scikit-learnâ€™s transformer API
class TextPreprocessor(BaseEstimator, TransformerMixin):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self  # No fitting required for this transformer

    def transform(self, X, y=None):
        return X.apply(self._preprocess_text)

    def _preprocess_text(self, text):
        if pd.isnull(text):
            return ''  # Return empty string for missing values
        text = text.lower()  # Lowercase
        text = ''.join([char for char in text if char not in string.punctuation])  # Remove punctuation
        tokens = word_tokenize(text)  # Tokenize
        tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords
        tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatize
        return ' '.join(tokens)  # Join tokens back into string


# Instantiate TextPreprocessor
text_preprocessor = TextPreprocessor()

# Apply the text preprocessor to 'title' and 'body' columns
data['processed_title'] = text_preprocessor.transform(data['title'])
data['processed_body'] = text_preprocessor.transform(data['body'])

# Display the processed data
data[['processed_title', 'processed_body']]






# TF-IDF -give higher weights to words that are less common across documents.
# Import necessary libraries
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import csr_matrix
import pandas as pd

# Apply TF-IDF to the title and body columns separately
tfidf_vectorizer = TfidfVectorizer(max_features=10000)  # limit to top 10,000 features_to manage vocabulary size


# Use the processed title and body columns 
tfidf_title = tfidf_vectorizer.fit_transform(data['processed_title'])
tfidf_body = tfidf_vectorizer.fit_transform(data['processed_body'])

# You can keep these as sparse matrices or concatenate directly if needed for model input
combined_features = csr_matrix(tfidf_title) + csr_matrix(tfidf_body) # in sparse format, to save memory














#Examine the top words by TF-IDF Scores

# Display the top terms with the highest TF-IDF scores
tfidf_sum = combined_features.sum(axis=0)
feature_names = tfidf_vectorizer.get_feature_names_out()
tfidf_df = pd.DataFrame({'term': feature_names, 'tfidf_score': tfidf_sum.A1})
tfidf_df = tfidf_df.sort_values(by='tfidf_score', ascending=False)

# Display the top 20 terms with the highest TF-IDF scores
tfidf_df.head(50)



# Visualize the Top TF-IDF Terms

# Bar plot of the top 20 terms
plt.figure(figsize=(12, 8))
plt.barh(tfidf_df['term'].head(20), tfidf_df['tfidf_score'].head(20))
plt.xlabel("TF-IDF Score")
plt.title("Top 20 TF-IDF Terms")
plt.gca().invert_yaxis()
plt.show()


# Calculate density (fraction of non-zero elements)
density_title = tfidf_title.nnz / (tfidf_title.shape[0] * tfidf_title.shape[1])
density_body = tfidf_body.nnz / (tfidf_body.shape[0] * tfidf_body.shape[1])
print(f"Density of TF-IDF matrix for title: {density_title:.4f}")
print(f"Density of TF-IDF matrix for body: {density_body:.4f}")






from sklearn.cluster import KMeans

# Define number of clusters
kmeans = KMeans(n_clusters=5, random_state=42)
clusters = kmeans.fit_predict(combined_features)

# Add the cluster labels to the original dataset for further analysis
data['cluster'] = clusters
print(data[['title', 'body', 'cluster']].head())






#Select and train a model using features extracted in the previous step.
# Model 1 - Baseline Model 
#Logistic Regression 

#Model 2 - Naive Bayes 

#Model 3 - SVM 

#Deep Learning BERT

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score

# Define the numerical and text features
numerical_features = ['upvotes', 'num_comments', 'year', 'month', 'day', 'hour', 'upvotes_per_comment', 'has_body']


# Define a transformer for scaling numerical features
preprocessor = ColumnTransformer([
    ('text', TfidfVectorizer(max_features=10000), 'combined_text'),  # Text processing
    ('num', StandardScaler(), ['upvotes', 'num_comments', 'upvotes_per_comment'])  # Scaling numerical columns
])

# For numerical features, we will impute missing values and then scale
numerical_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values (mean strategy)
    ('scaler', StandardScaler())  # Scale numerical features
])

# Combine transformers into a ColumnTransformer
preprocessor = ColumnTransformer([
    ('text', text_transformer, 'combined_text'),  # Apply text preprocessing to combined text
    ('numerical', numerical_transformer, numerical_features),  # Apply numerical transformations    
])

# Define the full pipeline with preprocessor and classifier
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(solver='saga', max_iter=200))  # You can switch this with any classifier
])

# Combine the title and body text into one column
data['combined_text'] = data['title'].fillna('') + ' ' + data['body'].fillna('')

# Set up features (X) and target (y)
X = data[['combined_text'] + numerical_features ]  # Include text and numerical features
y = data['label']  # Replace 'label' with the actual target column name

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the pipeline on the training data
pipeline.fit(X_train, y_train)

# Predictions on the training set
train_predictions = pipeline.predict(X_train)
train_accuracy = accuracy_score(y_train, train_predictions)
print("Training Accuracy:", train_accuracy)

# Predictions on the testing set
test_predictions = pipeline.predict(X_test)
test_accuracy = accuracy_score(y_test, test_predictions)
print("Testing Accuracy:", test_accuracy)









# Define a transformer for scaling numerical features
preprocessor = ColumnTransformer([
    ('text', TfidfVectorizer(max_features=10000), 'combined_text'),  # Text processing    
])

# Combine transformers into a ColumnTransformer
preprocessor = ColumnTransformer([
    ('text', text_transformer, 'combined_text'),  # Apply text preprocessing to combined text
    ])

# Define the full pipeline with preprocessor and classifier
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(solver='saga', max_iter=200))  # You can switch this with any classifier
])

# Combine the title and body text into one column
data['combined_text'] = data['title'].fillna('') + ' ' + data['body'].fillna('')

# Set up features (X) and target (y)
X = data[['combined_text']]  # Include text and numerical features
y = data['label']  # Replace 'label' with the actual target column name

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the pipeline on the training data
pipeline.fit(X_train, y_train)

# Predictions on the training set
train_predictions = pipeline.predict(X_train)
train_accuracy = accuracy_score(y_train, train_predictions)
print("Training Accuracy:", train_accuracy)

# Predictions on the testing set
test_predictions = pipeline.predict(X_test)
test_accuracy = accuracy_score(y_test, test_predictions)
print("Testing Accuracy:", test_accuracy)



#Model 2 - Naive Bayes 

#Model 3 - SVM 

#Deep Learning BERT










# Importing relevant libraries
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA

from textblob import TextBlob
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string
import re
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from scipy.sparse import csr_matrix


# Loading the dataset
df = pd.read_csv(r"D:\moringa_school\CapStone\final_project\Data\reduced_reddit.csv")
df.head()


# Statistical Summary
df.describe()


df.info()








# Checking for missing values in percentage
missing = (df.isnull().sum()/len(df))*100
missing


# Filling in for missing values in columns
# The 'Body' column with empty strings and the 'num_comments' with the median

df['num_comments'] = df['num_comments'].fillna(df['num_comments'].median())
df['body'] = df['body'].fillna('')


# Checking for missing values
df.isnull().sum()





# Checking for duplicates
df.duplicated().sum()





# Converting created_utc to datetime format 
df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')
df['created_utc']





# Plot distribution for the labels(Target variable)
sns.countplot(x='label', data=df, palette='Set1')
plt.title('Label Distribution')
plt.xlabel('Label')
plt.ylabel('Frequency')
plt.xticks(ticks=[0, 1], labels=['Non-depressive(0)', 'Depressive(1)'])
plt.show()


# Plot distribution for upvotes
sns.histplot(df['upvotes'], bins=50, kde=True, color='blue')
plt.title('Distribution of Upvotes')
plt.xlabel('Upvotes')
plt.ylabel('Count')
plt.show()


# Plot distribution for Number of Comments 
sns.histplot(df['num_comments'], bins=50, kde=True, color='green')
plt.title('Distribution of Number of Comments')
plt.xlabel('Number of Comments')
plt.ylabel('Count')
plt.show()


# Plot for correlation matrix of numerical features
plt.figure(figsize=(8, 6))
corr = df[['upvotes', 'num_comments', 'label']].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=1)
plt.title('Correlation Matrix')
plt.show()


# Plot distribution for Subreddit vs Number of posts 
plt.figure(figsize=(10, 6))
sns.countplot(y='subreddit', data=df, order=df['subreddit'].value_counts().index, palette='Set2')
plt.title('Post Distribution Across Subreddits')
plt.xlabel('Number of Posts')
plt.ylabel('Sub-reddit')
plt.show()


# Converting 'created_utc' to date-time format
df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')
df.set_index('created_utc', inplace=True)

# Plot trend for number of posts over time
df.resample('M').size().plot(figsize=(12, 6))
plt.title('Number of Posts Over Time')
plt.xlabel('Year')
plt.ylabel('Number of Posts')
plt.show()


# Boxplot for upvotes and label
sns.boxplot(x='label', y='upvotes', data=df, palette='coolwarm')
plt.title('Upvotes Distribution by Label')
plt.xlabel('Label')
plt.ylabel('Upvotes')
plt.xticks(ticks=[0, 1], labels=['Non Depressive', 'Depressive'])
plt.show()


# Scatter Plot: Upvotes vs Number of Comments
sns.scatterplot(x='upvotes', y='num_comments', hue='label', data=df, alpha=0.7, palette='Set2')
plt.title('Upvotes vs Number of Comments Distribution by Label')
plt.ylabel('Number of Comments')
plt.xlabel('Upvotes')
plt.legend(title='Label', labels=['Non depressive', 'Depressive'])
plt.show()
# Building correlation matrix
correlation_matrix = df[['upvotes', 'num_comments']].corr()

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)
plt.title("Correlation Matrix between Upvotes and Number of Comments")
plt.show()





# Text Preprocessing 
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    # Tokenize
    tokens = word_tokenize(text)
    # Remove stopwords and lemmatize
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

# Applying text preprocessing to title and body
df['processed_title'] = df['title'].apply(preprocess_text)
df['processed_body'] = df['body'].apply(preprocess_text)





# Using text blob for semtiment analysis
df['title_polarity'] = df['title'].apply(lambda x: TextBlob(x).sentiment.polarity)
df['body_polarity'] = df['body'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Length of title and body
df['title_word_count'] = df['title'].apply(lambda x: len(str(x).split()))
df['body_word_count'] = df['body'].apply(lambda x: len(str(x).split()))


# Plotting the distribution of polarity for the title and body
plt.figure(figsize=(12, 6))
sns.histplot(df['title_polarity'], kde=True, color='blue', label='Title Polarity')
sns.histplot(df['body_polarity'], kde=True, color='green', label='Body Polarity')
plt.title('Polarity Distribution for Title and Body')
plt.xlabel('Polarity')
plt.ylabel('Frequency')
plt.legend()
plt.show()


# Carrying out TF-IDF Vectorization for the body and title columns
tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')

tfidf_title = tfidf_vectorizer.fit_transform(df['processed_title']) 
tfidf_body = tfidf_vectorizer.fit_transform(df['processed_body'])

# Combine the features for both title and body
combined_features = csr_matrix(tfidf_title) + csr_matrix(tfidf_body)

























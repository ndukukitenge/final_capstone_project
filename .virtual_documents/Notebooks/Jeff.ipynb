





























import pandas as pd
import numpy as np
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.manifold import TSNE
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt', quiet=True)
np.random.seed(0)

nltk.download('stopwords')
nltk.download('wordnet')



df=pd.read_csv(r"C:\Users\user\Documents\final_project\Data\reduced_reddit.csv")
df


df.tail(10)


#there are a total of 98826 features,null values are in body and num_comment
#we will drop null values
df.info()


df=df.dropna()
df.info()





#lower casing the text
df['title'] = df['title'].str.lower().copy()
low_column=df[['subreddit','body']]
for col in low_column:
    df[col]=df[col].str.lower()




import re
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import unicodedata


#preprocessing
def preprocessing_text(text):
    #remove punctuations
    text = text.translate(str.maketrans('','', string.punctuation))
    #remove numbers
    text= re.sub(r'd/+', '',text)
    #remove stopwords
    stop_word=set(stopwords.words('english'))
    text=' '.join([word for word in text.split() if word not in stop_word])
    #tokenization
    tokens=word_tokenize(text)
    #lemmatization
    lemitizer= WordNetLemmatizer()
    tokens= [lemitizer.lemmatize(word) for word in tokens]
    #rejoin token
    text=' '.join(tokens)
    #remove extra whitespace
    text=' '.join(text.split())
    return text
    


#preprocess the text 
df_pre=pd.DataFrame(df[['title','body','subreddit']].applymap(preprocessing_text))


#categorical columns
df_num=pd.DataFrame(df[['upvotes','num_comments','label']])


#removing emojis from the title column
import emoji
def remove_emoji(text):
    return emoji.replace_emoji(text,'')

df_pre=df_pre.applymap(remove_emoji)






# we will encode the title column with TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer

tf_idf_vectorizer = TfidfVectorizer()
tf_idf_vectorizer_matrix= tf_idf_vectorizer.fit_transform(df_pre)
#vect_array=tf_idf_vectorizer_matrix.toarray()
print(tf_idf_vectorizer_matrix.shape)
vect_names=tf_idf_vectorizer.get_feature_names_out()
print(vect_names)






# # ENCODE THE BODY COLUMN
# tf_idf_vectorizer = TfidfVectorizer()
# tf_idf_vectorizer_matrix= tf_idf_vectorizer.fit_transform.(df['body'])
# #vect_array=tf_idf_vectorizer_matrix.toarray()
# print(tf_idf_vectorizer_matrix.shape)
# vect_names=tf_idf_vectorizer.get_feature_names_out()
# print(vect_names)





#GET THE TOP WORDS FROM 
# Sum up the TF-IDF scores across all documents for each word
tfidf_sums = tf_idf_vectorizer_matrix.sum(axis=0).A1 

# Get the top N indices of the terms with the highest TF-IDF scores
top_n = 10  # Adjust N as needed
top_n_indices = np.argsort(tfidf_sums)[::-1][:top_n]

# Get the top N words based on their TF-IDF sum
top_n_words = [vect_names[i] for i in top_n_indices]

# Print the top N words and their corresponding TF-IDF scores
top_n_scores = tfidf_sums[top_n_indices]
for word, score in zip(top_n_words, top_n_scores):
    print(f"{word}: {score}")



#FEATURE ENGINEER
from textblob import TextBlob
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import nltk
nltk.download('vader_lexicon')

# TextBlob 
def sentiment_score_textblob(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity  # Range: -1 (negative) to +1 (positive)

# VADER 
def sentiment_score_vader(text):
    sid = SentimentIntensityAnalyzer()
    score = sid.polarity_scores(text)
    return score['compound']  # Compound score combines positive/negative into one score


# text = df['title']
# print(sentiment_score_textblob(text))  # TextBlob sentiment score
# print(sentiment_score_vader(text))     # VADER compound sentiment score

sentiment_score= df_pre.head(10).applymap(sentiment_score_vader)
sentiment_score_blob= df_pre.head(10).applymap(sentiment_score_textblob)
print("vader sentiment score:",sentiment_score)
print("sentiment score blob:",sentiment_score_blob)







#word cloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt

def word_cloud(text):
    df_cloud = " ".join(text.tolist())
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(df_cloud)

    # Display the word cloud
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')  # Turn off axis
    plt.show()




# wordcloud for combined title and body columns
text = ' '.join(df_pre['title'].astype(str) + ' ' + df_pre['body'].astype(str))

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()



word_cloud(df['subreddit'])


word_cloud(df['body'])


#Topic modelling by using Latent Dirichlet Allocation
from sklearn.feature_extraction.text import CountVectorizer
from gensim import corpora
from gensim.models import LdaModel

df_pre['combined_text'] = df_pre['title'].astype(str) + ' ' + df_pre['body'].astype(str) + ' ' + df_pre['subreddit'].astype(str)
tokenized_text = df_pre['combined_text'].apply(lambda x: x.split()) 


#create dictionary for LDA
dictionary= corpora.Dictionary(tokenized_text)
corpus= [dictionary.doc2bow(text) for text in tokenized_text]
num_topic= 3
lda_model=LdaModel(corpus,num_topics=num_topic,id2word=dictionary,passes=10)

for idx , topic in lda_model.print_topics(-1):
    print(f'Topic{idx+1}:{topic}')





import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Concatenate DataFrames
df_model = pd.concat([df_pre, df_num], axis=1)

# Allocate X and y
y = df_model['label']
X = df_model[['title', 'body']].astype(str)  # Convert X to string format to avoid type issues

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Concatenate 'title' and 'body' columns as a single text for vectorization
X_train_text = X_train['title'] + ' ' + X_train['body']
X_test_text = X_test['title'] + ' ' + X_test['body']

# Vectorize the text data
vectorizer = TfidfVectorizer()
X_train_vect = vectorizer.fit_transform(X_train_text)  # Fit and transform training data
X_test_vect = vectorizer.transform(X_test_text)        # Transform test data

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(),
    "Naive Bayes": MultinomialNB(),
    "Support Vector Machine": SVC(kernel='linear'),
    "Decision Tree": DecisionTreeClassifier()
}

# Train, predict, and evaluate each model
for model_name, model in models.items():
    model.fit(X_train_vect, y_train)
    y_pred = model.predict(X_test_vect)  # Use X_test_vect instead of X_test
    print(f".......{model_name}......")
    print("Accuracy score:", accuracy_score(y_test, y_pred))
    print(classification_report(y_test, y_pred))
    print("\n")

